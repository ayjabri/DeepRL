{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b88b9d3",
   "metadata": {},
   "source": [
    "# Training Chatbots with RL\n",
    "\n",
    "**Objective:**  \n",
    "Train an entertainment human-mimicking chatbot using RL\n",
    "\n",
    "**Overview:**  \n",
    "A natural language model has been a science fiction thing for a long time. The objective of this notebook is to train a chatbot to respond back to a simple dialoge, not \"Luke Skywalker\" talking to his ship! :)\n",
    "The idea is based on a recurrent modle published back in 2014 (https://arxiv.org/abs/1406.6247). Although the papaer addresses reducing the computational expense of convolutional networks in image classificuations and objects detection, its implementation in NLP is huge!  \n",
    "The agent observes information in a local region instead of looking at the whole environent as traditional CNN agents do. It then applies sequential decesion process to learn how to better to focus its attention on areas that provides the best results....(continue with the paper)   \n",
    "\n",
    "## RNNs\n",
    "A unique feature of NLPs that sets them apart from computer vision and other domains, is thier ability to process variable-length inputs. i.e. words, sentences and paragraphs. A second feature is to understand a sequence of inputs and make one or a sequence of predictions.  \n",
    "Recurrent Neural Networks can do both. Even though they have fixed input and output, they can apply that to a sequence of inputs and pass information along this sequence in the form of hidden-state, which has fixed shape (size).   \n",
    "\n",
    "Refer to http://cs224d.stanford.edu/ for more information about RNN\n",
    "\n",
    "## Embedding\n",
    "Think of embedding layers as an advanced v-lookup that maps each word to a fixed length tensor. These tensors are trained on a large set of vocabulary, phrases and even abbreviations. This is the practical replacement of one-hot encoding as it solves the following problems:  \n",
    "1. Our vocabulary dataset could contain hundreds of thousands of words, which makes our input extremely large.\n",
    "2. Uneven frequency of vocabulary: in one-hot encoding common and uncommon words get the same representation which makes our use of space very inefficient.\n",
    "3. Word's relations: synonomus and similar words will be represented differently. Other words often used together like the \"United States\" and \"good job\"  \n",
    "     \n",
    "There are many ways to use embedding in your model. You can either download a pretrained set for the language you're using (GloVe or word2vec), or initiate randomly and train them on your own dataset which what i'm going to do\n",
    "\n",
    "## Encoder-Decoder model:\n",
    "This is the workhorse of every translating engine in NLP. It consists of two parts; the first part takes an input of N sequence of words in language A, then sequeeze it into a representation of fixed-length (encoder). The second part (decoder) takes this representation and generates an output from it in language B. This is called seq2seq model.\n",
    "\n",
    "## RL in seq2seq:\n",
    "Our decoder outputs the probability distribution at each step, which is very similar to policy gradient model. From this perspective we can consider our decoder an agent trying to select which action (token) to use at each step.  \n",
    "This is actually good because by sampling from that distribution gives more variaties and appear more human like.  For exmaple: there are many responses to the question \"hi, how are you?\". You can say \"I'm good, thank you?\" or \"I'm not too bad\", or even \"I'm not good\"..etc. They are all correct! Finding the average of all valid answers and then sampling from that distribution makes our agent's responses appear more natural!\n",
    "\n",
    "## Log Likelihood training:\n",
    "Take the example of training an French-English translator using seq2seq training. The obvious approach is to feed the French sentence into the encoder. Get its tensor representation (hidden-state of the last layer). And then feed it to the decoder along with the first token $<BEG>$ to get a probability distribution of the first word in our sentence   \n",
    "\n",
    "\n",
    "\n",
    "## The Algorithm\n",
    "The Reinforce method has the objective of maximizing  \n",
    "$ùîºùë†~ùëë,ùëé~ùúã(ùëé|ùë†)ùëÑ(ùë†,ùëé)logùúã(ùëé|ùë†).$  \n",
    "1. For every sample in the dataset, obtain th encoded representation, E, using the encoder RNN\n",
    "2. Initialize the current token with the special begin token $T= <BEG>$\n",
    "3. Initialize the output sequence withthe empty sequence $Out=[]$\n",
    "4. While $T \\neq <END>$:\n",
    "    * Get teh probability distribution of the tokens and the new hidden state, passing the current token and the hidden state: *p,hn=Decoder(T,E)*\n",
    "    * Sample the output token, $T_{out}$, from the probability distribution  \n",
    "    * Remember the probability distribution $p$  \n",
    "    * Append $T_{out}$ to the output sequence: $Out += T_{out}$\n",
    "    * Set the current token $T \\leftarrow T_{out}, E \\leftarrow H$\n",
    "5. Calculate BLEU or another metric between Out and the reference sequences: $Q = BLEU(Out,Out_{ref})$\n",
    "6. Estimate the gradients: $\\nabla J = \\sum_T Q\\nabla \\log p(T)$\n",
    "7. Update the model using SGD\n",
    "8. Repeat until conversion\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b556f792",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce8a28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
